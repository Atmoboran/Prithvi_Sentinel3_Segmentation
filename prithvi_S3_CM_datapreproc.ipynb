{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b4bacc318390456b",
      "metadata": {
        "id": "b4bacc318390456b"
      },
      "source": [
        "# Setup\n",
        "1. In colab: Go to \"Runtime\" -> \"Change runtime type\" -> Select \"T4 GPU\"\n",
        "2. Install TerraTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W_4z81Fn9RET",
      "metadata": {
        "id": "W_4z81Fn9RET"
      },
      "outputs": [],
      "source": [
        "!pip install terratorch==1.0.1 gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
      "metadata": {
        "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import gdown\n",
        "import terratorch\n",
        "import albumentations\n",
        "import lightning.pytorch as pl\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from terratorch.datamodules import GenericNonGeoSegmentationDataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "917b65b8e7cd7d65",
      "metadata": {
        "id": "917b65b8e7cd7d65"
      },
      "source": [
        "3. Download the dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcccf799-b8f6-494f-b576-c3f96f581fb1",
      "metadata": {
        "id": "fcccf799-b8f6-494f-b576-c3f96f581fb1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/terratorch_S3_CM'  # safer to write in 'MyDrive'\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "\n",
        "# --- CONFIG ---\n",
        "shared_url = 'https://drive.google.com/file/d/1JeY917uXpGrHTyuWLvA5A8n4VG2us8gs/view?usp=sharing'\n",
        "zip_path = \"patches_zip.zip\"\n",
        "extract_tmp = \"/content/temp_extracted\"\n",
        "final_output_dir = os.path.join(base_dir, \"downloaded_data\")\n",
        "# --------------\n",
        "\n",
        "def download_and_unzip(url, zip_file_path, extract_to, move_to):\n",
        "    if os.path.exists(zip_file_path):\n",
        "        print(f\"‚úÖ Found existing file: {zip_file_path}. Skipping download.\")\n",
        "    else:\n",
        "        print(f\"‚¨áÔ∏è Downloading from {url} ...\")\n",
        "        gdown.download(url, zip_file_path, quiet=False, fuzzy=True)\n",
        "\n",
        "    print(f\"üì¶ Extracting to {extract_to} ...\")\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    print(f\"üìÅ Moving extracted files to: {move_to}\")\n",
        "    os.makedirs(move_to, exist_ok=True)\n",
        "    for filename in os.listdir(extract_to):\n",
        "        shutil.move(os.path.join(extract_to, filename), move_to)\n",
        "\n",
        "    shutil.rmtree(extract_to)  # clean up\n",
        "    print(\"‚úÖ Done.\")\n",
        "\n",
        "download_and_unzip(shared_url, zip_path, extract_tmp, final_output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbSQESc_g8Sg"
      },
      "id": "dbSQESc_g8Sg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60768a89-bae4-4533-946c-d963d6aec15b",
      "metadata": {
        "id": "60768a89-bae4-4533-946c-d963d6aec15b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def merge_and_split_by_date(download_root, output_root, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    source_folders = [\n",
        "        os.path.join(download_root, name) for name in os.listdir(download_root)\n",
        "        if os.path.isdir(os.path.join(download_root, name))\n",
        "    ]\n",
        "\n",
        "    # Shuffle and split source folders (by date)\n",
        "    random.seed(43)  # or any number you like\n",
        "    random.shuffle(source_folders)\n",
        "\n",
        "    total = len(source_folders)\n",
        "    train_end = int(train_ratio * total)\n",
        "    val_end = train_end + int(val_ratio * total)\n",
        "\n",
        "    split_map = {\n",
        "        \"train\": source_folders[:train_end],\n",
        "        \"val\": source_folders[train_end:val_end],\n",
        "        \"test\": source_folders[val_end:]\n",
        "    }\n",
        "\n",
        "    for split, folders in split_map.items():\n",
        "        split_dir = os.path.join(output_root, f\"{split}-data\")\n",
        "        os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "        reflectance_files = []\n",
        "        current_index_offset = 0\n",
        "\n",
        "        for folder in folders:\n",
        "            files = sorted(os.listdir(folder))\n",
        "            reflectance = [f for f in files if f.endswith(\"_reflectance.tif\")]\n",
        "\n",
        "            for filename in reflectance:\n",
        "                parts = filename.split(\"_\")\n",
        "                original_index = int(parts[1])\n",
        "                new_index = current_index_offset + original_index\n",
        "                timestamp = parts[0]\n",
        "                patch_size = parts[2]\n",
        "\n",
        "                new_name = f\"{timestamp}_{new_index:04d}_{patch_size}_reflectance.tif\"\n",
        "                shutil.copy(os.path.join(folder, filename), os.path.join(split_dir, new_name))\n",
        "                reflectance_files.append(new_name)\n",
        "\n",
        "            for filename in files:\n",
        "                if filename.endswith(\"_binary.tif\"):\n",
        "                    parts = filename.split(\"_\")\n",
        "                    original_index = int(parts[1])\n",
        "                    new_index = current_index_offset + original_index\n",
        "                    timestamp = parts[0]\n",
        "                    patch_size = parts[2]\n",
        "\n",
        "                    new_name = f\"{timestamp}_{new_index:04d}_{patch_size}_binary.tif\"\n",
        "                    shutil.copy(os.path.join(folder, filename), os.path.join(split_dir, new_name))\n",
        "\n",
        "            current_index_offset = len(reflectance_files)\n",
        "\n",
        "        print(f\"‚úÖ {split.upper()} set: Merged and renamed {len(reflectance_files)} reflectance files into {split_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efaf4f8-ad91-4731-bdc2-a20ff36dd6f5",
      "metadata": {
        "id": "1efaf4f8-ad91-4731-bdc2-a20ff36dd6f5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def create_split_files_from_existing_dirs(data_root, splits_dir):\n",
        "    os.makedirs(splits_dir, exist_ok=True)\n",
        "\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        split_dir = os.path.join(data_root, f\"{split}-data\")\n",
        "        if not os.path.isdir(split_dir):\n",
        "            print(f\"‚ö†Ô∏è Missing split directory: {split_dir}\")\n",
        "            continue\n",
        "\n",
        "        base_names = []\n",
        "        for filename in os.listdir(split_dir):\n",
        "            if filename.endswith(\"_reflectance.tif\"):\n",
        "                base = filename.replace(\"_reflectance.tif\", \"\")\n",
        "                base_names.append(base)\n",
        "\n",
        "        base_names = sorted(list(set(base_names)))\n",
        "\n",
        "        with open(os.path.join(splits_dir, f\"{split}.txt\"), \"w\") as f:\n",
        "            f.writelines(f\"{name}\\n\" for name in base_names)\n",
        "\n",
        "        print(f\"‚úÖ {split}.txt created with {len(base_names)} entries.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ba72c4-65eb-4826-bfcf-66d32903c363",
      "metadata": {
        "id": "09ba72c4-65eb-4826-bfcf-66d32903c363"
      },
      "outputs": [],
      "source": [
        "import rasterio\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "def balance_dataset_by_capping(data_dir,split,threshold = 0.6):\n",
        "    binary_files = [f for f in os.listdir(data_dir) if f.endswith(\"_binary.tif\")]\n",
        "    patch_stats = []\n",
        "    ratios = []\n",
        "\n",
        "    os.makedirs(f\"{base_dir}/plots/data_hists\", exist_ok=True)\n",
        "\n",
        "    print(f\"\\nüîç Scanning {data_dir} for class balance...\")\n",
        "\n",
        "    for fname in binary_files:\n",
        "        path = os.path.join(data_dir, fname)\n",
        "        with rasterio.open(path) as src:\n",
        "            mask = src.read(1)\n",
        "\n",
        "        count_0 = np.sum(mask == 0)\n",
        "        count_1 = np.sum(mask == 1)\n",
        "        total = count_0 + count_1\n",
        "\n",
        "        if total == 0:\n",
        "            continue\n",
        "\n",
        "        ratio = count_1 / total\n",
        "        ratios.append(ratio)\n",
        "\n",
        "        patch_stats.append({\n",
        "            \"filename\": fname,\n",
        "            \"count_0\": count_0,\n",
        "            \"count_1\": count_1,\n",
        "            \"total\": total,\n",
        "            \"ratio\": ratio\n",
        "        })\n",
        "\n",
        "    # Plot histogram before balancing\n",
        "    bins = np.arange(0, 1.1, 0.1)\n",
        "    plt.hist(ratios, bins=bins, color=\"skyblue\", edgecolor=\"black\", align='mid', rwidth=0.8)\n",
        "    plt.title(f\"Class 1 ratio per patch (before balancing) - {os.path.basename(data_dir)}\")\n",
        "    plt.xlabel(\"Proportion of Class 1 pixels\")\n",
        "    plt.ylabel(\"Number of patches\")\n",
        "    plt.xticks(bins)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{base_dir}/plots/data_hists/Hist_before_balancing_{split.replace('-data','')}.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Filter out high-ratio patches for mean calculation\n",
        "    filtered_ratios = [r for r in ratios if r <= threshold] # important threshold !\n",
        "    mean_ratio = np.mean(filtered_ratios)\n",
        "    print(f\"üìä Mean ratio (<={threshold}): {mean_ratio:.4f}\")\n",
        "\n",
        "    # Bin patches by rounded ratio\n",
        "    ratio_bins = defaultdict(list)\n",
        "    for patch in patch_stats:\n",
        "        bin_key = min(np.floor(patch[\"ratio\"] * 10) / 10, 0.9)\n",
        "        ratio_bins[bin_key].append(patch)\n",
        "\n",
        "    mean_bin = round(mean_ratio, 1)\n",
        "    print('mean bin', mean_bin)\n",
        "    max_bin_size = len(ratio_bins[mean_bin])\n",
        "    print(f\"üì¶ Max patches per ratio bin (based on mean bin): {max_bin_size}\")\n",
        "\n",
        "    kept_patches = []\n",
        "    deleted = 0\n",
        "\n",
        "    for bin_key, patches in ratio_bins.items():\n",
        "        if len(patches) > max_bin_size:\n",
        "            random.shuffle(patches)\n",
        "            kept = patches[:max_bin_size]\n",
        "            deleted_patches = patches[max_bin_size:]\n",
        "            for patch in deleted_patches:\n",
        "                fname = patch[\"filename\"]\n",
        "                base = fname.replace(\"_binary.tif\", \"\")\n",
        "                binary_path = os.path.join(data_dir, fname)\n",
        "                refl_path = os.path.join(data_dir, base + \"_reflectance.tif\")\n",
        "\n",
        "                os.remove(binary_path)\n",
        "                os.remove(refl_path)\n",
        "                deleted += 1\n",
        "        else:\n",
        "            kept = patches\n",
        "\n",
        "        kept_patches.extend(kept)\n",
        "\n",
        "    final_ratios = [p[\"ratio\"] for p in kept_patches]\n",
        "\n",
        "    plt.hist(final_ratios, bins=bins, color=\"lightcoral\", edgecolor=\"black\", align='mid', rwidth=0.8)\n",
        "    plt.title(f\"Class 1 ratio per patch (after capping) - {os.path.basename(data_dir)}\")\n",
        "    plt.xlabel(\"Proportion of Class 1 pixels\")\n",
        "    plt.ylabel(\"Number of patches\")\n",
        "    plt.xticks(bins)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{base_dir}/plots/data_hists/Hist_after_balancing_{split.replace('-data','')}.png\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"üóë Deleted {deleted} patches from {os.path.basename(data_dir)}\")\n",
        "    print(f\"‚úÖ Final dataset size: {len(kept_patches)} patches\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd70191-c5d2-446f-8f34-2e4dbd8fc689",
      "metadata": {
        "id": "2fd70191-c5d2-446f-8f34-2e4dbd8fc689"
      },
      "outputs": [],
      "source": [
        "download_root = base_dir+\"/downloaded_data\"\n",
        "merged_base = base_dir + \"/merged\"\n",
        "output_root = os.path.join(merged_base, \"data\")\n",
        "os.makedirs(output_root, exist_ok = True)\n",
        "\n",
        "# Delete all subdirectories in output_root\n",
        "for entry in os.listdir(output_root):\n",
        "    path = os.path.join(output_root, entry)\n",
        "    if os.path.isdir(path):\n",
        "        shutil.rmtree(path)\n",
        "        print(f\"üóëÔ∏è Deleted directory: {path}\")\n",
        "\n",
        "merge_and_split_by_date(download_root, output_root, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
        "\n",
        "# Apply to all splits\n",
        "data_root = output_root\n",
        "for split in [\"train-data\", \"val-data\", \"test-data\"]:\n",
        "    split_dir = os.path.join(data_root, split)\n",
        "    if os.path.isdir(split_dir):\n",
        "        balance_dataset_by_capping(split_dir, split, threshold = 0.5)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "splits_dir = merged_base+\"/data/splits\"\n",
        "create_split_files_from_existing_dirs(data_root, splits_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260b806b-a3c2-4fdc-8353-e6677d348212",
      "metadata": {
        "id": "260b806b-a3c2-4fdc-8353-e6677d348212"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import rasterio\n",
        "\n",
        "# Computing the mean and stds only on the training data is best practise to avoid information leakage.\n",
        "\n",
        "# Directory containing the reflectance images\n",
        "data_dir = merged_base + \"/data/train-data\"\n",
        "output_path = os.path.join(os.path.dirname(data_dir), \"means_stds.txt\")\n",
        "\n",
        "# Find all *_reflectance.tif files\n",
        "file_paths = glob.glob(os.path.join(data_dir, \"*_reflectance.tif\"))\n",
        "\n",
        "# Initialize accumulators\n",
        "band_sums = np.zeros(6, dtype=np.float64)\n",
        "band_squared_sums = np.zeros(6, dtype=np.float64)\n",
        "pixel_counts = np.zeros(6, dtype=np.int64)\n",
        "\n",
        "# Loop through files and compute running sums\n",
        "for path in file_paths:\n",
        "    with rasterio.open(path) as src:\n",
        "        img = src.read()  # Shape: (bands, height, width)\n",
        "\n",
        "        if img.shape[0] != 6:\n",
        "            print(f\"‚ö†Ô∏è Skipping {path} ‚Äî expected 6 bands, got {img.shape[0]}\")\n",
        "            continue\n",
        "\n",
        "        # Flatten each band and accumulate stats\n",
        "        for b in range(6):\n",
        "            band_data = img[b].astype(np.float64)\n",
        "\n",
        "            if np.isnan(band_data).any():\n",
        "                print(f\"‚ö†Ô∏è NaNs found in file: {path}, band: {b}\")\n",
        "\n",
        "            mask = ~np.isnan(band_data)  # Mask out NaNs\n",
        "            band_sums[b] += band_data[mask].sum()\n",
        "            band_squared_sums[b] += (band_data[mask] ** 2).sum()\n",
        "            pixel_counts[b] += mask.sum()\n",
        "\n",
        "# Calculate per-band mean and std\n",
        "means = band_sums / pixel_counts\n",
        "stds = np.sqrt(band_squared_sums / pixel_counts - means ** 2)\n",
        "\n",
        "# Save to file\n",
        "with open(output_path, \"w\") as f:\n",
        "    f.write(\"Band\\tMean\\t\\tStd\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    for i in range(6):\n",
        "        f.write(f\"{i+1}\\t{means[i]:.6f}\\t{stds[i]:.6f}\\n\")\n",
        "\n",
        "print(f\"\\n‚úÖ Per-band statistics saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb8eb4af-0653-457f-9511-3bb6e41daa3d",
      "metadata": {
        "id": "fb8eb4af-0653-457f-9511-3bb6e41daa3d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "terratorch_env",
      "language": "python",
      "name": "terratorch_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}