{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "247838fe-8d67-4cc2-86bb-0a93c7cf97fb",
      "metadata": {
        "id": "247838fe-8d67-4cc2-86bb-0a93c7cf97fb"
      },
      "source": [
        "# Prithvi Sen3 CM Model config and training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fcc3453-17d2-4ec9-b3fd-9a5d6799d27e",
      "metadata": {
        "id": "1fcc3453-17d2-4ec9-b3fd-9a5d6799d27e"
      },
      "source": [
        "Run prithvi_S3_CM_datapreproc.ipynb before in order to create the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install terratorch==1.0.1"
      ],
      "metadata": {
        "id": "IseURL3b5CxA"
      },
      "id": "IseURL3b5CxA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
      "metadata": {
        "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import gdown\n",
        "import terratorch\n",
        "import albumentations\n",
        "import lightning.pytorch as pl\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
      "metadata": {
        "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
      },
      "source": [
        "First we create and analyze the datamodule!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8bc3941-b9af-403d-9348-e347b5561058",
      "metadata": {
        "id": "a8bc3941-b9af-403d-9348-e347b5561058"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/terratorch_S3_CM/'\n",
        "\n",
        "dataset_path = Path(base_dir+'/merged/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b04d13d-afc5-47f1-9aaf-8b57bd7041e1",
      "metadata": {
        "id": "7b04d13d-afc5-47f1-9aaf-8b57bd7041e1"
      },
      "outputs": [],
      "source": [
        "# Load means and stds\n",
        "with open(dataset_path / 'data/means_stds.txt') as f:\n",
        "    lines = f.readlines()[2:]  # Skip header and separator\n",
        "    stats = [tuple(map(float, line.strip().split()[1:])) for line in lines]\n",
        "\n",
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
      "metadata": {
        "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
      },
      "outputs": [],
      "source": [
        "datamodule = terratorch.datamodules.GenericNonGeoSegmentationDataModule(\n",
        "    batch_size=8,\n",
        "    num_workers=2,\n",
        "    num_classes=2,\n",
        "\n",
        "    # Define dataset paths\n",
        "    train_data_root=dataset_path / 'data/train-data',\n",
        "    train_label_data_root=dataset_path / 'data/train-data',\n",
        "    val_data_root=dataset_path / 'data/val-data',\n",
        "    val_label_data_root=dataset_path / 'data/val-data',\n",
        "    test_data_root=dataset_path / 'data/test-data',\n",
        "    test_label_data_root=dataset_path / 'data/test-data',\n",
        "\n",
        "    # Define splits\n",
        "    train_split=dataset_path / 'data/splits/train.txt',\n",
        "    val_split=dataset_path / 'data/splits/val.txt',\n",
        "    test_split=dataset_path / 'data/splits/test.txt',\n",
        "\n",
        "    img_grep='*_reflectance.tif',\n",
        "    label_grep='*_binary.tif',\n",
        "\n",
        "    train_transform=[\n",
        "        albumentations.D4(), # Random flips and rotation\n",
        "        albumentations.pytorch.transforms.ToTensorV2(),\n",
        "    ],\n",
        "    val_transform=None,  # Using ToTensor() by default\n",
        "    test_transform=None,\n",
        "\n",
        "    # Define standardization values\n",
        "    means=[\n",
        "      stats[0][0],\n",
        "      stats[1][0],\n",
        "      stats[2][0],\n",
        "      stats[3][0],\n",
        "      stats[4][0],\n",
        "      stats[5][0],\n",
        "    ],\n",
        "    stds=[\n",
        "      stats[0][1],\n",
        "      stats[1][1],\n",
        "      stats[2][1],\n",
        "      stats[3][1],\n",
        "      stats[4][1],\n",
        "      stats[5][1],\n",
        "    ],\n",
        "    no_data_replace = 0,\n",
        "    no_label_replace = -1,\n",
        "    # We use all six bands of the data, so we don't need to define dataset_bands and output_bands.\n",
        ")\n",
        "\n",
        "# Setup train and val datasets\n",
        "datamodule.setup(\"fit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60623b2c-41b0-4b50-9eb8-03febd4e7afc",
      "metadata": {
        "id": "60623b2c-41b0-4b50-9eb8-03febd4e7afc"
      },
      "outputs": [],
      "source": [
        "datamodule.setup(\"fit\")\n",
        "train_dataset = datamodule.train_dataset\n",
        "val_dataset = datamodule.val_dataset\n",
        "\n",
        "datamodule.setup(\"test\")\n",
        "test_dataset = datamodule.test_dataset\n",
        "\n",
        "print(\"Train:\", len(train_dataset))\n",
        "print(\"Val:  \", len(val_dataset))\n",
        "print(\"Test: \", len(test_dataset))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e653821-2ffe-4942-a75c-8c617932873b",
      "metadata": {
        "id": "7e653821-2ffe-4942-a75c-8c617932873b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_class_distribution(dataset, name=\"dataset\"):\n",
        "    pixel_counter = Counter()\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        mask = dataset[i]['mask']\n",
        "        if isinstance(mask, torch.Tensor):\n",
        "            mask = mask.squeeze().cpu().numpy()  # Remove channel dim if exists\n",
        "        unique, counts = np.unique(mask, return_counts=True)\n",
        "        pixel_counter.update(dict(zip(unique, counts)))\n",
        "\n",
        "    total_pixels = sum(pixel_counter.values())\n",
        "\n",
        "    print(f\"\\n📊 Class distribution in '{name}':\")\n",
        "    for label, count in sorted(pixel_counter.items()):\n",
        "        percentage = (count / total_pixels) * 100\n",
        "        print(f\"Class {int(label)}: {count} pixels ({percentage:.2f}%)\")\n",
        "\n",
        "    return pixel_counter\n",
        "\n",
        "# Analyze each split\n",
        "train_counts = analyze_class_distribution(train_dataset, name=\"train\")\n",
        "val_counts = analyze_class_distribution(val_dataset, name=\"val\")\n",
        "test_counts = analyze_class_distribution(test_dataset, name=\"test\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ce3ce0-0d48-4d46-bf0c-ab2deb6dcf60",
      "metadata": {
        "id": "93ce3ce0-0d48-4d46-bf0c-ab2deb6dcf60"
      },
      "outputs": [],
      "source": [
        "# check if there is any overlap in the data\n",
        "train_ids = set(sample[\"filename\"] for sample in train_dataset)\n",
        "val_ids = set(sample[\"filename\"] for sample in val_dataset)\n",
        "test_ids = set(sample[\"filename\"] for sample in test_dataset)\n",
        "\n",
        "overlap_train_val = train_ids & val_ids\n",
        "overlap_train_test = train_ids & test_ids\n",
        "overlap_val_test = val_ids & test_ids\n",
        "\n",
        "with open(\"overlap_report.txt\", \"w\") as f:\n",
        "    f.write(\"Train ∩ Val:\\n\")\n",
        "    f.writelines(f\"{fn}\\n\" for fn in sorted(overlap_train_val))\n",
        "    f.write(\"\\nTrain ∩ Test:\\n\")\n",
        "    f.writelines(f\"{fn}\\n\" for fn in sorted(overlap_train_test))\n",
        "    f.write(\"\\nVal ∩ Test:\\n\")\n",
        "    f.writelines(f\"{fn}\\n\" for fn in sorted(overlap_val_test))\n",
        "\n",
        "print(\"✅ Saved overlaps to overlap_report.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da0330d-f047-416b-bbdb-f3bba3f032ee",
      "metadata": {
        "id": "8da0330d-f047-416b-bbdb-f3bba3f032ee"
      },
      "outputs": [],
      "source": [
        "# plotting a few samples\n",
        "print('Train Plots')\n",
        "train_dataset.plot(train_dataset[1])\n",
        "train_dataset.plot(train_dataset[4])\n",
        "train_dataset.plot(train_dataset[8])\n",
        "# plotting a few samples\n",
        "print('Val Plots')\n",
        "val_dataset.plot(val_dataset[1])\n",
        "val_dataset.plot(val_dataset[4])\n",
        "val_dataset.plot(val_dataset[8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
      "metadata": {
        "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
      },
      "outputs": [],
      "source": [
        "# checking datasets testing split size\n",
        "datamodule.setup(\"test\")\n",
        "test_dataset = datamodule.test_dataset\n",
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654a30ddef8ed5a",
      "metadata": {
        "id": "654a30ddef8ed5a"
      },
      "source": [
        "# Fine-tune Prithvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe2b822-2861-479b-aca1-3b1a012b61a4",
      "metadata": {
        "id": "3fe2b822-2861-479b-aca1-3b1a012b61a4"
      },
      "outputs": [],
      "source": [
        "# first we create a dictionary, so that we can run through different configurations:\n",
        "model_configs = {\n",
        "    \"exp_0\": {\n",
        "        \"exp_no\": 0,\n",
        "        \"backbone\": \"prithvi_eo_v2_300\",\n",
        "        \"backbone_pretrained\": False,\n",
        "        \"backbone_encoding\": [],\n",
        "        \"freeze_backbone\": False,\n",
        "        \"freeze_decoder\": False,\n",
        "        \"max_epochs\": 5,\n",
        "    },\n",
        "\n",
        "    \"exp_1\": {\n",
        "        \"exp_no\": 1,\n",
        "        \"backbone\": \"prithvi_eo_v2_300\",\n",
        "        \"backbone_pretrained\": False,\n",
        "        \"backbone_encoding\": [],\n",
        "        \"freeze_backbone\": True,\n",
        "        \"freeze_decoder\": False,\n",
        "        \"max_epochs\": 20,\n",
        "    },\n",
        "\n",
        "    \"exp_2\": {\n",
        "        \"exp_no\": 2,\n",
        "        \"backbone\": \"prithvi_eo_v2_300\",\n",
        "        \"backbone_pretrained\": True,\n",
        "        \"backbone_encoding\": [],\n",
        "        \"freeze_backbone\": False,\n",
        "        \"freeze_decoder\": False,\n",
        "        \"max_epochs\": 20,\n",
        "    },\n",
        "\n",
        "    \"exp_3\": {\n",
        "        \"exp_no\": 3,\n",
        "        \"backbone\": \"prithvi_eo_v2_300\",\n",
        "        \"backbone_pretrained\": True,\n",
        "        \"backbone_encoding\": [],\n",
        "        \"freeze_backbone\": True,\n",
        "        \"freeze_decoder\": False,\n",
        "        \"max_epochs\": 20,\n",
        "    },\n",
        "\n",
        "    \"exp_4\": {\n",
        "        \"exp_no\": 4,\n",
        "        \"backbone\": \"prithvi_eo_v2_300_tl\",\n",
        "        \"backbone_pretrained\": True,\n",
        "        \"backbone_encoding\": ['time','location'],\n",
        "        \"freeze_backbone\": True,\n",
        "        \"freeze_decoder\": False,\n",
        "        \"max_epochs\": 20,\n",
        "    },\n",
        "\n",
        "    \"exp_5\": {\n",
        "        \"exp_no\": 5,\n",
        "        \"backbone\": \"prithvi_eo_v2_600\",\n",
        "        \"backbone_pretrained\": False,\n",
        "        \"backbone_encoding\": [],\n",
        "        \"freeze_backbone\": False,\n",
        "        \"freeze_decoder\": False,\n",
        "        \"max_epochs\": 20,\n",
        "    },\n",
        "\n",
        "    \"exp_6\": {\n",
        "        \"exp_no\": 6,\n",
        "        \"backbone\": \"prithvi_eo_v2_600\",\n",
        "        \"backbone_pretrained\": True,\n",
        "        \"backbone_encoding\": [],\n",
        "        \"freeze_backbone\": True,\n",
        "        \"freeze_decoder\": False,\n",
        "        \"max_epochs\": 20,\n",
        "    },\n",
        "    \"exp_7\": {\n",
        "        \"exp_no\": 7,\n",
        "        \"backbone\": \"prithvi_eo_v2_300\",\n",
        "        \"backbone_pretrained\": False,\n",
        "        \"backbone_encoding\": [],\n",
        "        \"freeze_backbone\": True,\n",
        "        \"freeze_decoder\": True,\n",
        "        \"max_epochs\": 20,\n",
        "    },\n",
        "    \"exp_31\": {\n",
        "        \"exp_no\": 31,\n",
        "        \"backbone\": \"prithvi_eo_v2_300\",\n",
        "        \"backbone_pretrained\": False,\n",
        "        \"backbone_encoding\": [],\n",
        "        \"freeze_backbone\": True,\n",
        "        \"freeze_decoder\": False,\n",
        "        \"max_epochs\": 150,\n",
        "    },\n",
        "\n",
        "    \"exp_33\": {\n",
        "        \"exp_no\": 33,\n",
        "        \"backbone\": \"prithvi_eo_v2_300\",\n",
        "        \"backbone_pretrained\": True,\n",
        "        \"backbone_encoding\": [],\n",
        "        \"freeze_backbone\": True,\n",
        "        \"freeze_decoder\": False,\n",
        "        \"max_epochs\": 150,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    # Add more experiments here...\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame.from_dict(model_configs, orient=\"index\")\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"model_configs.csv\", index_label=\"experiment\")\n",
        "\n",
        "print(\"✅ Saved: model_configs.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb84a457-262a-46e9-82cd-e34ff2971ac4",
      "metadata": {
        "id": "eb84a457-262a-46e9-82cd-e34ff2971ac4"
      },
      "source": [
        "# These functions have to be defined before running the fine-tuning loop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef87c36-a910-43a1-a08c-0c12455b9b40",
      "metadata": {
        "id": "1ef87c36-a910-43a1-a08c-0c12455b9b40"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def generate_uncertainty_maps(model, test_loader, exp_no, save_dir=f\"{base_dir}/plots/uncertainty_art\"):\n",
        "    save_dir = os.path.join(save_dir, f\"exp_{exp_no}\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "    for batch_idx, batch in enumerate(test_loader):\n",
        "        with torch.no_grad():\n",
        "            images = batch[\"image\"].to(model.device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Get probabilities\n",
        "            probs = torch.softmax(outputs.output, dim=1)\n",
        "            probs_np = probs.cpu().numpy()\n",
        "\n",
        "            # Compute pixel-wise entropy (uncertainty)\n",
        "            entropy = -np.sum(probs_np * np.log(probs_np + 1e-10), axis=1)\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.imshow(entropy[i], cmap='magma')\n",
        "            plt.axis(\"off\")\n",
        "            plt.tight_layout(pad=0)\n",
        "\n",
        "            filename = f\"exp_{exp_no}_batch_{batch_idx}_sample_{i}_uncertainty.png\"\n",
        "            save_path = os.path.join(save_dir, filename)\n",
        "            plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "            plt.close()\n",
        "\n",
        "    print(f\"✅ Saved all uncertainty maps to: {save_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8759d88c-349f-4b4c-b8f7-37da85580305",
      "metadata": {
        "id": "8759d88c-349f-4b4c-b8f7-37da85580305"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_experiment_metrics(exp_no):\n",
        "    # === Find highest version folder ===\n",
        "    exp_path = f\"{base_dir}/logs/exp_{exp_no}\"\n",
        "    versions = [d for d in os.listdir(exp_path) if d.startswith(\"version_\")]\n",
        "    version_nums = [int(v.split(\"_\")[1]) for v in versions]\n",
        "    latest_version = f\"version_{max(version_nums)}\"\n",
        "    metrics_path = os.path.join(exp_path, latest_version, \"metrics.csv\")\n",
        "\n",
        "    # === Load CSV ===\n",
        "    df = pd.read_csv(metrics_path)\n",
        "\n",
        "    # === Step-level loss plot ===\n",
        "    step_df = df.dropna(subset=[\"step\", \"train/loss\"])\n",
        "    grouped = step_df.groupby(\"step\").mean()\n",
        "    has_val_loss = \"val/loss\" in grouped.columns and not grouped[\"val/loss\"].isnull().all()\n",
        "\n",
        "    # === Epoch-level average loss ===\n",
        "    val_df = df[df[\"val/loss\"].notna()].drop_duplicates(subset=\"epoch\")\n",
        "    train_df = df[[\"epoch\", \"train/loss\"]].dropna()\n",
        "    avg_train = train_df.groupby(\"epoch\").mean().rename(columns={\"train/loss\": \"avg_train_loss\"})\n",
        "    plot_df = val_df.merge(avg_train, on=\"epoch\")\n",
        "\n",
        "    # === Plotting ===\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    # Plot 1: Step-wise loss\n",
        "    axes[0].plot(grouped.index, grouped[\"train/loss\"], label=\"Train Loss\", linewidth=2)\n",
        "    if has_val_loss:\n",
        "        axes[0].plot(grouped.index, grouped[\"val/loss\"], label=\"Val Loss\", linewidth=2)\n",
        "    axes[0].set_ylim(0, 0.3)  # ✅ Fixed y-limits for loss\n",
        "    axes[0].set_title(\"Loss Per Step\")\n",
        "    axes[0].set_xlabel(\"Step\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Plot 2: Epoch-wise loss\n",
        "    axes[1].plot(plot_df[\"epoch\"], plot_df[\"avg_train_loss\"], label=\"Train Loss (avg)\", marker=\"o\")\n",
        "    axes[1].plot(plot_df[\"epoch\"], plot_df[\"val/loss\"], label=\"Val Loss\", marker=\"o\")\n",
        "    axes[1].set_ylim(0, 0.3)  # ✅ Fixed y-limits for loss\n",
        "    axes[1].set_title(\"Loss Per Epoch\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].set_ylabel(\"Loss\")\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # Plot 3: Validation metrics\n",
        "    val_metrics = [\n",
        "        col for col in plot_df.columns\n",
        "        if col.startswith(\"val/\") and \"loss\" not in col and plot_df[col].notna().any()\n",
        "    ]\n",
        "    # Plot 3: Validation metrics\n",
        "    for metric in val_metrics:\n",
        "        short_name = metric.replace(\"val/\", \"\").replace(\"_\", \" \")\n",
        "        axes[2].plot(plot_df[\"epoch\"], plot_df[metric], label=short_name, marker=\"o\")\n",
        "    axes[2].set_ylim(0.6, 1.0)  # ✅ Fixed y-limits for val metrics\n",
        "\n",
        "\n",
        "    axes[2].set_title(\"Validation Metrics Per Epoch\")\n",
        "    axes[2].set_xlabel(\"Epoch\")\n",
        "    axes[2].set_ylabel(\"Metric Value\")\n",
        "    axes[2].legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.35), ncol=2)\n",
        "    axes[2].grid(True)\n",
        "\n",
        "    # Save and show\n",
        "    plt.tight_layout()\n",
        "    save_path = f\"{base_dir}/plots/loss_metrics/exp_{exp_no}_combined_loss_and_metrics.png\"\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "    print(f\"✅ Saved: {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a805b2a5-c8b6-453e-bed1-f2ba559f79a5",
      "metadata": {
        "id": "a805b2a5-c8b6-453e-bed1-f2ba559f79a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "from sklearn.metrics import jaccard_score\n",
        "import matplotlib.colors as mcolors\n",
        "from itertools import islice\n",
        "\n",
        "def dice_coefficient(pred, target, smooth=1e-6):\n",
        "    intersection = np.sum(pred * target)\n",
        "    return (2. * intersection + smooth) / (np.sum(pred) + np.sum(target) + smooth)\n",
        "\n",
        "def compute_jaccard_index(pred, target):\n",
        "    return jaccard_score(target.flatten(), pred.flatten(), average='binary')\n",
        "\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "def extract_timestamps(filenames):\n",
        "    \"\"\"Extract timestamps like 20230510T101427 from a list of filenames.\"\"\"\n",
        "    timestamps = []\n",
        "    for fname in filenames:\n",
        "        match = re.search(r'\\d{8}T\\d{6}', Path(fname).name)\n",
        "        timestamps.append(match.group(0) if match else None)\n",
        "    return timestamps\n",
        "\n",
        "\n",
        "def visualize_full_segmentation_analysis(exp_no, best_ckpt_path, model, datamodule, target_class=1,\n",
        "                                          save_dir=f\"{base_dir}/plots/full_analysis\", max_rows_per_fig=4, amount_batches_to_plot ='all'):\n",
        "    save_dir = os.path.join(save_dir, f\"exp_{exp_no}\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # --- Load Model from checkpoint ---\n",
        "  #  model = model.__class__.load_from_checkpoint(\n",
        "   #     best_ckpt_path,\n",
        "    #    model_factory=model.hparams.model_factory,\n",
        "     #   model_args=model.hparams.model_args,\n",
        "    #)\n",
        "\n",
        "    trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)\n",
        "\n",
        "    model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
        "        best_ckpt_path,\n",
        "        model_factory=model.hparams.model_factory,\n",
        "        model_args=model.hparams.model_args,\n",
        "    )\n",
        "\n",
        "\n",
        "    # --- Load test dataloader ---\n",
        "    test_loader = datamodule.test_dataloader()\n",
        "\n",
        "\n",
        "    if amount_batches_to_plot == 'all':\n",
        "\n",
        "        amount_batches_to_plot = len(test_loader)\n",
        "\n",
        "    # inialize list\n",
        "\n",
        "    jaccard_list = []\n",
        "    dice_list = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(islice(test_loader, amount_batches_to_plot)):\n",
        "        print(f\"Processing batch {batch_idx}...\")\n",
        "\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "                    images = datamodule.aug(batch)\n",
        "                    images = batch[\"image\"].to(model.device)\n",
        "                    filenames = batch[\"filename\"]\n",
        "                    timestamps = extract_timestamps(filenames)\n",
        "                    outputs = model(images)\n",
        "\n",
        "                    probs = torch.softmax(outputs.output, dim=1).cpu().numpy()\n",
        "                    preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
        "\n",
        "        images_np = images.cpu().numpy()\n",
        "        gts = batch[\"mask\"].numpy() if \"mask\" in batch else np.zeros_like(preds)\n",
        "\n",
        "        num_images = len(images)\n",
        "        cols = 5\n",
        "        num_parts = math.ceil(num_images / max_rows_per_fig)\n",
        "\n",
        "        for part_idx in range(num_parts):\n",
        "            start_idx = part_idx * max_rows_per_fig\n",
        "            end_idx = min((part_idx + 1) * max_rows_per_fig, num_images)\n",
        "            current_rows = end_idx - start_idx\n",
        "\n",
        "            fig, axes = plt.subplots(current_rows, cols, figsize=(cols * 4, current_rows * 3))\n",
        "\n",
        "            for i in range(start_idx, end_idx):\n",
        "                row = i - start_idx\n",
        "                row_axes = axes[row] if current_rows > 1 else axes\n",
        "\n",
        "                img = images_np[i][0]\n",
        "                img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
        "\n",
        "                gt_mask = gts[i]\n",
        "                pred_mask = preds[i]\n",
        "                diff_mask = gt_mask - pred_mask\n",
        "                prob_map = probs[i, target_class, :, :]\n",
        "\n",
        "                # --- Metrics ---\n",
        "                dice =  1 - dice_coefficient(pred_mask == target_class, gt_mask == target_class)\n",
        "                jaccard = compute_jaccard_index(pred_mask == target_class, gt_mask == target_class)\n",
        "\n",
        "                dice_list.append(dice)\n",
        "                jaccard_list.append(jaccard)\n",
        "\n",
        "                # Track the Jaccard index and loss\n",
        "                #total_jaccard += jaccard\n",
        "                # Assuming that the model outputs raw logits or a similar format:\n",
        "                #v#al_loss = dice  # You can modify this to whatever loss you are calculating\n",
        "                #total_loss += val_loss\n",
        "\n",
        "                titles = [\n",
        "                    f\"Sen3/SLSTR Band2 (659nm)\\n{timestamps[i]}\",\n",
        "                    \"Reference Mask\",\n",
        "                    f\"Prediction\\nDice Loss: {dice:.3f}, Jaccard: {jaccard:.3f}\",\n",
        "                    \"difference(Ref - Pred)\",\n",
        "                    f\"Prob. Class {target_class}\"\n",
        "                ]\n",
        "\n",
        "                visuals = [img, gt_mask, pred_mask, diff_mask, prob_map]\n",
        "\n",
        "                # Define custom colormap for the input image\n",
        "                cmap_input = mcolors.LinearSegmentedColormap.from_list(\"\", [\"black\", \"blue\", \"red\", \"yellow\"])\n",
        "\n",
        "                # Define colormap for the gt_mask and pred_mask (discrete colors: blue and white)\n",
        "                cmap_mask = mcolors.ListedColormap(['blue', 'white'])\n",
        "\n",
        "                # Define the colormap for the other images\n",
        "                #cmaps = [cmap_input, cmap_mask, cmap_mask, \"bwr\", \"viridis\"]\n",
        "                cmaps = [\"cubehelix\", cmap_mask, cmap_mask, \"bwr\", \"viridis\"]\n",
        "\n",
        "                # Define the vmin and vmax for each image\n",
        "                vmins = [0, 0, 0, -1, 0]\n",
        "                vmaxs = [1, 1, 1, 1, 1]\n",
        "\n",
        "                # Loop through columns and plot images with appropriate colormap and colorbars\n",
        "                for j in range(cols):\n",
        "                    ax = row_axes[j]\n",
        "                    im = ax.imshow(visuals[j], cmap=cmaps[j], vmin=vmins[j], vmax=vmaxs[j])\n",
        "                    ax.set_title(titles[j])\n",
        "                    ax.grid(True, linestyle=\":\", linewidth=0.5)\n",
        "\n",
        "                    # Add colorbar based on the visual type\n",
        "                    if j == 1 or j == 2:  # For gt_mask and pred_mask, use the discrete colormap\n",
        "                        fig.colorbar(im, ax=ax, ticks=[0, 1], fraction=0.046, pad=0.04)\n",
        "                    else:\n",
        "                        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            filename = f\"exp_{exp_no}_batch_{batch_idx}_part_{part_idx}.png\"\n",
        "            save_path = os.path.join(save_dir, filename)\n",
        "            plt.savefig(save_path)\n",
        "            plt.close()\n",
        "            print(f\"✅ Saved: {save_path}\")\n",
        "\n",
        "\n",
        "\n",
        "    # After all batches, calculate mean Jaccard index and mean loss\n",
        "    mean_jaccard = sum(jaccard_list) / len(jaccard_list)\n",
        "    mean_loss = sum(dice_list) / len(dice_list)\n",
        "\n",
        "\n",
        "    # Save the metrics to a text file\n",
        "    metrics_file = os.path.join(save_dir, \"metrics.txt\")\n",
        "    with open(metrics_file, \"w\") as f:\n",
        "        f.write(f\"Mean Jaccard Index: {mean_jaccard:.4f}\\n\")\n",
        "        f.write(f\"Mean Validation Loss: {mean_loss:.4f}\\n\")\n",
        "        f.write(f\"Jaccard list: {jaccard_list}\\n\")\n",
        "        f.write(f\"Dice list: {dice_list}\\n\")\n",
        "    print(f\"✅ Metrics saved to: {metrics_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf618672-b220-4c04-bdb8-b731029ed8c6",
      "metadata": {
        "id": "cf618672-b220-4c04-bdb8-b731029ed8c6"
      },
      "source": [
        "# TRAINING in a Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e97a42-2307-43bc-98eb-c5d6b34d5882",
      "metadata": {
        "scrolled": true,
        "collapsed": true,
        "id": "35e97a42-2307-43bc-98eb-c5d6b34d5882"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "results_file = \"experiment_results.txt\"\n",
        "\n",
        "# Only create and write header if the file does not exist\n",
        "if not os.path.exists(results_file):\n",
        "    with open(results_file, \"w\") as f:\n",
        "        f.write(\"Experiment Results:\\n\")\n",
        "\n",
        "for config_name, cfg in list(model_configs.items())[0:1]:   ## CHANGE HERE FOR ALL\n",
        "    print(f\"\\n🚀 Running {config_name}...\")\n",
        "\n",
        "\n",
        "    pl.seed_everything(0)\n",
        "    ####################\n",
        "    exp_no = cfg['exp_no']\n",
        "    ####################\n",
        "\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "        dirpath=f\"{base_dir}/output/sen3_cm/checkpoints/exp_{exp_no}\",\n",
        "        mode=\"max\",\n",
        "        monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
        "        filename=\"best-{epoch:02d}\",\n",
        "        save_top_k=1\n",
        "    )\n",
        "\n",
        "\n",
        "    from pytorch_lightning.loggers import CSVLogger\n",
        "    logger = CSVLogger(f\"{base_dir}/logs/\", name=f\"exp_{exp_no}\")\n",
        "\n",
        "\n",
        "    # Lightning Trainer\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator=\"auto\",\n",
        "        strategy=\"auto\",\n",
        "        devices=1, # Deactivate multi-gpu because it often fails in notebooks\n",
        "        precision='16-mixed',  # Speed up training\n",
        "        num_nodes=1,\n",
        "        logger=logger,\n",
        "        max_epochs=cfg[\"max_epochs\"],\n",
        "        log_every_n_steps=20,\n",
        "        enable_checkpointing=True,\n",
        "        callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
        "        default_root_dir=f\"{base_dir}/output/sen3_cm\",\n",
        "    )\n",
        "\n",
        "    if cfg[\"backbone\"].startswith(\"prithvi_eo_v2_100\"):\n",
        "        indices = [2, 5, 8, 11]\n",
        "    elif cfg[\"backbone\"].startswith(\"prithvi_eo_v2_300\"):\n",
        "        indices = [5, 11, 17, 23]\n",
        "    elif cfg[\"backbone\"].startswith(\"prithvi_eo_v2_600\"):\n",
        "        indices = [7, 15, 23, 31]\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported backbone: {cfg['backbone']}\")\n",
        "\n",
        "    # Model\n",
        "    model = terratorch.tasks.SemanticSegmentationTask(\n",
        "        model_factory=\"EncoderDecoderFactory\",\n",
        "        model_args={\n",
        "            # Backbone\n",
        "            \"backbone\": cfg[\"backbone\"],\n",
        "            \"backbone_pretrained\": cfg[\"backbone_pretrained\"],\n",
        "            \"backbone_num_frames\": 1, # 1 is the default value\n",
        "            \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"], #this is actually not true when using Sen3 data\n",
        "            \"backbone_coords_encoding\": cfg[\"backbone_encoding\"],\n",
        "\n",
        "            # Necks\n",
        "            \"necks\": [\n",
        "                {\"name\": \"SelectIndices\",\n",
        "                 \"indices\": indices},\n",
        "                {\"name\": \"ReshapeTokensToImage\",},\n",
        "                {\"name\": \"LearnedInterpolateToPyramidal\"}\n",
        "            ],\n",
        "\n",
        "            # Decoder\n",
        "            \"decoder\": \"UNetDecoder\",\n",
        "            \"decoder_channels\": [512, 256, 128, 64],\n",
        "\n",
        "            # Head\n",
        "            \"head_dropout\": 0.1,\n",
        "            \"num_classes\": 2,\n",
        "        },\n",
        "\n",
        "        loss=\"dice\",\n",
        "        optimizer=\"AdamW\",\n",
        "        lr=1e-4,\n",
        "        ignore_index=-1,\n",
        "        freeze_backbone=cfg[\"freeze_backbone\"],\n",
        "        freeze_decoder=cfg[\"freeze_decoder\"],\n",
        "        plot_on_val=True,\n",
        "        class_names=['clear-sky ocean', 'else']  # optionally define class names\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    trainer.fit(model, datamodule=datamodule)\n",
        "    print(f\"✅ Best checkpoint: {checkpoint_callback.best_model_path}\")\n",
        "    print(f\"🏆 Best score: {checkpoint_callback.best_model_score}\")\n",
        "\n",
        "    with open(results_file, \"a\") as f:\n",
        "        f.write(f\"\\n📁 Experiment: {config_name}\\n\")\n",
        "        f.write(f\"Best Checkpoint: {checkpoint_callback.best_model_path}\\n\")\n",
        "        f.write(f\"Best Score: {checkpoint_callback.best_model_score:.4f}\\n\")\n",
        "\n",
        "    plot_experiment_metrics(exp_no)\n",
        "    best_ckpt_path = checkpoint_callback.best_model_path\n",
        "\n",
        "    trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)\n",
        "    test_loader = datamodule.test_dataloader()\n",
        "    generate_uncertainty_maps(model, test_loader, exp_no)\n",
        "\n",
        "    visualize_full_segmentation_analysis(\n",
        "        exp_no,\n",
        "        best_ckpt_path,\n",
        "        model=model,\n",
        "        datamodule=datamodule\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99fadb72-10aa-4203-b02f-94075951cf5c",
      "metadata": {
        "id": "99fadb72-10aa-4203-b02f-94075951cf5c"
      },
      "source": [
        "# Here you can run the plots without training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51568790-801c-4a17-bfb1-4f7199f3b1ff",
      "metadata": {
        "id": "51568790-801c-4a17-bfb1-4f7199f3b1ff"
      },
      "source": [
        "comment: complete model definition might be unnecessary here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d4447c-e187-40be-9c9c-861cb173d28e",
      "metadata": {
        "collapsed": true,
        "id": "70d4447c-e187-40be-9c9c-861cb173d28e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "for i, (config_name, cfg) in enumerate(model_configs.items()):\n",
        "    print(config_name)\n",
        "    #if i in [0,1,2,3,4,5,6,7,8,9]:\n",
        "    if i in [0]:\n",
        "        print(f\"🚀 Running evaluation of {config_name}...\")\n",
        "\n",
        "        pl.seed_everything(0)\n",
        "        ####################\n",
        "        exp_no = cfg['exp_no']\n",
        "        ####################\n",
        "\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "            dirpath=f\"output/sen3_cm/checkpoints/exp_{exp_no}\",\n",
        "            mode=\"max\",\n",
        "            monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
        "            filename=\"best-{epoch:02d}\",\n",
        "            save_top_k=1\n",
        "        )\n",
        "\n",
        "\n",
        "        from pytorch_lightning.loggers import CSVLogger\n",
        "        logger = CSVLogger(\"logs/\", name=f\"exp_{exp_no}\")\n",
        "\n",
        "\n",
        "        # Lightning Trainer\n",
        "        trainer = pl.Trainer(\n",
        "            accelerator=\"auto\",\n",
        "            strategy=\"auto\",\n",
        "            devices=1, # Deactivate multi-gpu because it often fails in notebooks\n",
        "            precision='16-mixed',  # Speed up training\n",
        "            num_nodes=1,\n",
        "            logger=logger,\n",
        "            max_epochs=cfg[\"max_epochs\"],\n",
        "            log_every_n_steps=20,\n",
        "            enable_checkpointing=True,\n",
        "            callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
        "            default_root_dir=\"output/sen3_cm\",\n",
        "        )\n",
        "\n",
        "        if cfg[\"backbone\"].startswith(\"prithvi_eo_v2_100\"):\n",
        "            indices = [2, 5, 8, 11]\n",
        "        elif cfg[\"backbone\"].startswith(\"prithvi_eo_v2_300\"):\n",
        "            indices = [5, 11, 17, 23]\n",
        "        elif cfg[\"backbone\"].startswith(\"prithvi_eo_v2_600\"):\n",
        "            indices = [7, 15, 23, 31]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {cfg['backbone']}\")\n",
        "\n",
        "        # Model\n",
        "        model = terratorch.tasks.SemanticSegmentationTask(\n",
        "            model_factory=\"EncoderDecoderFactory\",\n",
        "            model_args={\n",
        "                # Backbone\n",
        "                \"backbone\": cfg[\"backbone\"],\n",
        "                \"backbone_pretrained\": cfg[\"backbone_pretrained\"],\n",
        "                \"backbone_num_frames\": 1, # 1 is the default value\n",
        "                \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"], #this is actually not true when using Sen3 data\n",
        "                \"backbone_coords_encoding\": cfg[\"backbone_encoding\"],\n",
        "\n",
        "                # Necks\n",
        "                \"necks\": [\n",
        "                    {\"name\": \"SelectIndices\",\n",
        "                     \"indices\": indices},\n",
        "                    {\"name\": \"ReshapeTokensToImage\",},\n",
        "                    {\"name\": \"LearnedInterpolateToPyramidal\"}\n",
        "                ],\n",
        "\n",
        "                # Decoder\n",
        "                \"decoder\": \"UNetDecoder\",\n",
        "                \"decoder_channels\": [512, 256, 128, 64],\n",
        "\n",
        "                # Head\n",
        "                \"head_dropout\": 0.1,\n",
        "                \"num_classes\": 2,\n",
        "            },\n",
        "\n",
        "            loss=\"dice\",\n",
        "            optimizer=\"AdamW\",\n",
        "            lr=1e-4,\n",
        "            ignore_index=-1,\n",
        "            freeze_backbone=cfg[\"freeze_backbone\"],\n",
        "            freeze_decoder=cfg[\"freeze_decoder\"],\n",
        "            plot_on_val=True,\n",
        "            class_names=['clear-sky ocean', 'else']  # optionally define class names\n",
        "        )\n",
        "\n",
        "        # === Find best .ckpt file (highest epoch number) ===\n",
        "        checkpoint_dir = f\"{base_dir}/output/sen3_cm/checkpoints/exp_{exp_no}\"\n",
        "        ckpt_files = glob.glob(os.path.join(checkpoint_dir, \"*.ckpt\"))\n",
        "\n",
        "        if not ckpt_files:\n",
        "            print(f\"⚠️ No checkpoint found in {checkpoint_dir}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        def extract_epoch(filename):\n",
        "            base = os.path.basename(filename)\n",
        "            if \"epoch=\" in base:\n",
        "                try:\n",
        "                    return int(base.split(\"epoch=\")[-1].split(\".\")[0])\n",
        "                except:\n",
        "                    return -1\n",
        "            return -1\n",
        "\n",
        "        best_ckpt_path = max(ckpt_files, key=extract_epoch)\n",
        "        print(f\"📦 Loading checkpoint: {best_ckpt_path}\")\n",
        "\n",
        "       # plot_experiment_metrics(exp_no)\n",
        "        trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)\n",
        "        #test_loader = datamodule.test_dataloader()\n",
        "        #generate_uncertainty_maps(model, test_loader, exp_no)\n",
        "\n",
        "\n",
        "        visualize_full_segmentation_analysis(\n",
        "            exp_no,\n",
        "            best_ckpt_path,\n",
        "            model=model,\n",
        "            datamodule=datamodule,\n",
        "            amount_batches_to_plot = 'all'\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdafff51-6641-43af-b48e-af93f03a9c33",
      "metadata": {
        "id": "bdafff51-6641-43af-b48e-af93f03a9c33"
      },
      "source": [
        "# compare different loss and metrics in ONE plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1039d6c9-e5ed-45eb-bc6a-8ab01ced826a",
      "metadata": {
        "id": "1039d6c9-e5ed-45eb-bc6a-8ab01ced826a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_multiple_experiment_metrics(exp_nos, metric=\"Multiclass_Jaccard_Index\"):\n",
        "    # === Initialize lists to store data for all experiments ===\n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    all_jaccard_indices = []\n",
        "\n",
        "    # To store min/max values for table export\n",
        "    min_train_losses = []\n",
        "    min_val_losses = []\n",
        "    max_jaccard_indices = []\n",
        "\n",
        "    # Loop over each experiment number and process\n",
        "    for exp_no in exp_nos:\n",
        "        print(f\"\\nProcessing experiment {exp_no}...\")\n",
        "\n",
        "        # === Find highest version folder ===\n",
        "        exp_path = f\"{base_dir}/logs/exp_{exp_no}\"\n",
        "        versions = [d for d in os.listdir(exp_path) if d.startswith(\"version_\")]\n",
        "        version_nums = [int(v.split(\"_\")[1]) for v in versions]\n",
        "        latest_version = f\"version_{max(version_nums)}\"\n",
        "        metrics_path = os.path.join(exp_path, latest_version, \"metrics.csv\")\n",
        "\n",
        "        # === Load CSV ===\n",
        "        df = pd.read_csv(metrics_path)\n",
        "\n",
        "        # === Epoch-level average loss ===\n",
        "        val_df = df[df[\"val/loss\"].notna()].drop_duplicates(subset=\"epoch\")\n",
        "        train_df = df[[\"epoch\", \"train/loss\"]].dropna()\n",
        "        avg_train = train_df.groupby(\"epoch\").mean().rename(columns={\"train/loss\": \"avg_train_loss\"})\n",
        "        plot_df = val_df.merge(avg_train, on=\"epoch\")\n",
        "\n",
        "        # Collect data for plotting\n",
        "        all_train_losses.append(plot_df[\"avg_train_loss\"])\n",
        "        all_val_losses.append(plot_df[\"val/loss\"])\n",
        "\n",
        "        # Collect Jaccard Index for validation\n",
        "        jaccard_index = plot_df.get(f\"val/{metric}\", None)\n",
        "        if jaccard_index is not None:\n",
        "            all_jaccard_indices.append(jaccard_index)\n",
        "        else:\n",
        "            all_jaccard_indices.append(None)\n",
        "\n",
        "        # Store min/max values for table\n",
        "        min_train_losses.append(min(plot_df[\"avg_train_loss\"]))\n",
        "        min_val_losses.append(min(plot_df[\"val/loss\"]))\n",
        "        if jaccard_index is not None:\n",
        "            max_jaccard_indices.append(max(jaccard_index))\n",
        "        else:\n",
        "            max_jaccard_indices.append(None)\n",
        "\n",
        "    # === Plotting ===\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(20, 15))\n",
        "\n",
        "    # Plot 1: All Train Losses\n",
        "    for i, train_loss in enumerate(all_train_losses):\n",
        "        axes[0].plot(train_loss, label=f\"Exp {exp_nos[i]} - Min Loss: {min_train_losses[i]:.4f}\")\n",
        "    axes[0].set_ylim(0, 0.3)  # Fixed y-limits for loss\n",
        "    axes[0].set_title(\"Train Loss (Dice) Per Epoch\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "    axes[0].legend(ncol=2)\n",
        "    axes[0].grid(True)\n",
        "    axes[0].set_xticks(range(len(all_train_losses[0])))  # Set x-ticks to match number of epochs\n",
        "    axes[0].set_xticklabels(range(1, len(all_train_losses[0]) + 1))  # Display epochs from 1 to n\n",
        "\n",
        "    # Plot 2: All Validation Losses\n",
        "    for i, val_loss in enumerate(all_val_losses):\n",
        "        axes[1].plot(val_loss, label=f\"Exp {exp_nos[i]} - Min Val Loss: {min_val_losses[i]:.4f}\")\n",
        "    axes[1].set_ylim(0, 0.3)  # Fixed y-limits for loss\n",
        "    axes[1].set_title(\"Validation Loss (Dice) Per Epoch\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].set_ylabel(\"Loss\")\n",
        "    axes[1].legend(ncol=2)\n",
        "    axes[1].grid(True)\n",
        "    axes[1].set_xticks(range(len(all_val_losses[0])))  # Set x-ticks to match number of epochs\n",
        "    axes[1].set_xticklabels(range(1, len(all_val_losses[0]) + 1))  # Display epochs from 1 to n\n",
        "\n",
        "    # Plot 3: All Multiclass Jaccard Indices\n",
        "    for i, jaccard_index in enumerate(all_jaccard_indices):\n",
        "        if jaccard_index is not None:\n",
        "            axes[2].plot(jaccard_index, label=f\"Exp {exp_nos[i]} - Max Jaccard: {max_jaccard_indices[i]:.4f}\")\n",
        "    axes[2].set_ylim(0.6, 1.0)  # Fixed y-limits for Jaccard Index\n",
        "    axes[2].set_title(\"Multiclass Jaccard Index Per Epoch\")\n",
        "    axes[2].set_xlabel(\"Epoch\")\n",
        "    axes[2].set_ylabel(\"Jaccard Index\")\n",
        "    axes[2].legend(ncol=2)\n",
        "    axes[2].grid(True)\n",
        "    axes[2].set_xticks(range(len(all_jaccard_indices[0])))  # Set x-ticks to match number of epochs\n",
        "    axes[2].set_xticklabels(range(1, len(all_jaccard_indices[0]) + 1))  # Display epochs from 1 to n\n",
        "\n",
        "    # Save and show\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = f\"{base_dir}/plots/comparison_{metric}_exp_{'_'.join(map(str, exp_nos))}_loss_and_metrics.png\"\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"✅ Saved: {save_path}\")\n",
        "\n",
        "    # === Save Min/Max Losses and Jaccard Indices to a .txt File ===\n",
        "    results_filename = f\"{base_dir}/plots/experiment_comparison_min_max_{'_'.join(map(str, exp_nos))}.txt\"\n",
        "    with open(results_filename, \"w\") as f:\n",
        "        f.write(f\"Experiment Comparison: Min Losses, Max Jaccard Indices\\n\")\n",
        "        for i, exp_no in enumerate(exp_nos):\n",
        "            f.write(f\"Exp {exp_no}: Min Train Loss: {min_train_losses[i]:.4f}, Min Val Loss: {min_val_losses[i]:.4f}, Max Jaccard Index: {max_jaccard_indices[i]:.4f}\\n\")\n",
        "\n",
        "    print(f\"✅ Min/Max values saved to: {results_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0529f7-da10-4d60-b6e5-e716b3bd2fc9",
      "metadata": {
        "id": "6c0529f7-da10-4d60-b6e5-e716b3bd2fc9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def plot_multiple_experiment_metrics(exp_nos, metric=\"Multiclass_Jaccard_Index\"):\n",
        "    # === Initialize lists to store data for all experiments ===\n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    all_jaccard_indices = []\n",
        "\n",
        "    # To store min/max values for table export\n",
        "    min_train_losses = []\n",
        "    min_val_losses = []\n",
        "    max_jaccard_indices = []\n",
        "\n",
        "    # Loop over each experiment number and process\n",
        "    for exp_no in exp_nos:\n",
        "        print(f\"\\nProcessing experiment {exp_no}...\")\n",
        "\n",
        "        # === Find highest version folder ===\n",
        "        exp_path = f\"{base_dir}/logs/exp_{exp_no}\"\n",
        "        versions = [d for d in os.listdir(exp_path) if d.startswith(\"version_\")]\n",
        "        version_nums = [int(v.split(\"_\")[1]) for v in versions]\n",
        "        latest_version = f\"version_{max(version_nums)}\"\n",
        "\n",
        "        # Find the metrics.csv file with the largest file size in the latest version\n",
        "        version_path = os.path.join(exp_path, latest_version)\n",
        "        files = [f for f in os.listdir(version_path) if f.endswith(\".csv\")]\n",
        "\n",
        "        largest_file = None\n",
        "        largest_size = 0\n",
        "        latest_version_file = None\n",
        "        latest_version_num = -1\n",
        "\n",
        "        # Check each file for size and keep track of the largest one\n",
        "        for file in files:\n",
        "            file_path = os.path.join(version_path, file)\n",
        "            file_size = os.path.getsize(file_path)  # Get file size in bytes\n",
        "\n",
        "            # If this file is larger than the current largest file, update\n",
        "            if file_size > largest_size:\n",
        "                largest_size = file_size\n",
        "                largest_file = file_path\n",
        "                # Also track the version for comparison in case of a tie\n",
        "                latest_version_file = file\n",
        "                latest_version_num = int(file.split(\"_\")[1].split(\".\")[0])\n",
        "            # If file sizes are equal, compare version numbers and choose the later one\n",
        "            elif file_size == largest_size:\n",
        "                version_num_from_file = int(file.split(\"_\")[1].split(\".\")[0])\n",
        "                if version_num_from_file > latest_version_num:\n",
        "                    largest_file = file_path\n",
        "                    latest_version_file = file\n",
        "                    latest_version_num = version_num_from_file\n",
        "\n",
        "        metrics_path = largest_file\n",
        "        print(f\"Chosen metrics file: {metrics_path}\")\n",
        "\n",
        "        # === Load CSV ===\n",
        "        df = pd.read_csv(metrics_path)\n",
        "\n",
        "        # === Epoch-level average loss ===\n",
        "        val_df = df[df[\"val/loss\"].notna()].drop_duplicates(subset=\"epoch\")\n",
        "        train_df = df[[\"epoch\", \"train/loss\"]].dropna()\n",
        "        avg_train = train_df.groupby(\"epoch\").mean().rename(columns={\"train/loss\": \"avg_train_loss\"})\n",
        "        plot_df = val_df.merge(avg_train, on=\"epoch\")\n",
        "\n",
        "        # Collect data for plotting\n",
        "        all_train_losses.append(plot_df[\"avg_train_loss\"])\n",
        "        all_val_losses.append(plot_df[\"val/loss\"])\n",
        "\n",
        "        # Collect Jaccard Index for validation\n",
        "        jaccard_index = plot_df.get(f\"val/{metric}\", None)\n",
        "        if jaccard_index is not None:\n",
        "            all_jaccard_indices.append(jaccard_index)\n",
        "        else:\n",
        "            all_jaccard_indices.append(None)\n",
        "\n",
        "        # Store min/max values for table\n",
        "        min_train_losses.append(min(plot_df[\"avg_train_loss\"]))\n",
        "        min_val_losses.append(min(plot_df[\"val/loss\"]))\n",
        "        if jaccard_index is not None:\n",
        "            max_jaccard_indices.append(max(jaccard_index))\n",
        "        else:\n",
        "            max_jaccard_indices.append(None)\n",
        "\n",
        "    # === Plotting ===\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(20, 15))\n",
        "\n",
        "    # Plot 1: All Train Losses\n",
        "    for i, train_loss in enumerate(all_train_losses):\n",
        "        axes[0].plot(train_loss, label=f\"Exp {exp_nos[i]} - Min Loss: {min_train_losses[i]:.4f}\")\n",
        "    axes[0].set_ylim(0, 0.3)  # Fixed y-limits for loss\n",
        "    axes[0].set_title(\"Train Loss (Dice) Per Epoch\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "    axes[0].legend(ncol=2)\n",
        "    axes[0].grid(True)\n",
        "    axes[0].set_xticks(range(len(all_train_losses[0])))  # Set x-ticks to match number of epochs\n",
        "    axes[0].set_xticklabels(range(1, len(all_train_losses[0]) + 1))  # Display epochs from 1 to n\n",
        "\n",
        "    # Plot 2: All Validation Losses\n",
        "    for i, val_loss in enumerate(all_val_losses):\n",
        "        axes[1].plot(val_loss, label=f\"Exp {exp_nos[i]} - Min Val Loss: {min_val_losses[i]:.4f}\")\n",
        "    axes[1].set_ylim(0, 0.3)  # Fixed y-limits for loss\n",
        "    axes[1].set_title(\"Validation Loss (Dice) Per Epoch\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].set_ylabel(\"Loss\")\n",
        "    axes[1].legend(ncol=2)\n",
        "    axes[1].grid(True)\n",
        "    axes[1].set_xticks(range(len(all_val_losses[0])))  # Set x-ticks to match number of epochs\n",
        "    axes[1].set_xticklabels(range(1, len(all_val_losses[0]) + 1))  # Display epochs from 1 to n\n",
        "\n",
        "    # Plot 3: All Multiclass Jaccard Indices\n",
        "    for i, jaccard_index in enumerate(all_jaccard_indices):\n",
        "        if jaccard_index is not None:\n",
        "            axes[2].plot(jaccard_index, label=f\"Exp {exp_nos[i]} - Max Jaccard: {max_jaccard_indices[i]:.4f}\")\n",
        "    axes[2].set_ylim(0.6, 1.0)  # Fixed y-limits for Jaccard Index\n",
        "    axes[2].set_title(\"Multiclass Jaccard Index Per Epoch\")\n",
        "    axes[2].set_xlabel(\"Epoch\")\n",
        "    axes[2].set_ylabel(\"Jaccard Index\")\n",
        "    axes[2].legend(ncol=2)\n",
        "    axes[2].grid(True)\n",
        "    axes[2].set_xticks(range(len(all_jaccard_indices[0])))  # Set x-ticks to match number of epochs\n",
        "    axes[2].set_xticklabels(range(1, len(all_jaccard_indices[0]) + 1))  # Display epochs from 1 to n\n",
        "\n",
        "    # Save and show\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = f\"{base_dir}/plots/comparison_{metric}_exp_{'_'.join(map(str, exp_nos))}_loss_and_metrics.png\"\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"✅ Saved: {save_path}\")\n",
        "\n",
        "    # === Save Min/Max Losses and Jaccard Indices to a .txt File ===\n",
        "    results_filename = f\"{base_dir}/plots/experiment_comparison_min_max_{'_'.join(map(str, exp_nos))}.txt\"\n",
        "    with open(results_filename, \"w\") as f:\n",
        "        f.write(f\"Experiment Comparison: Min Losses, Max Jaccard Indices\\n\")\n",
        "        for i, exp_no in enumerate(exp_nos):\n",
        "            f.write(f\"Exp {exp_no}: Min Train Loss: {min_train_losses[i]:.4f}, Min Val Loss: {min_val_losses[i]:.4f}, Max Jaccard Index: {max_jaccard_indices[i]:.4f}\\n\")\n",
        "\n",
        "    print(f\"✅ Min/Max values saved to: {results_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f943c4a3-0b13-43ed-95c1-9a853ef3bd58",
      "metadata": {
        "id": "f943c4a3-0b13-43ed-95c1-9a853ef3bd58"
      },
      "outputs": [],
      "source": [
        "plot_multiple_experiment_metrics([0, 1, 2,3,4,5,6,7,8,9])  # Compare experiments 0, 1, and 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a736577-b5a8-485e-8ac3-2783fe0487d5",
      "metadata": {
        "id": "6a736577-b5a8-485e-8ac3-2783fe0487d5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307e686b-5f41-471d-81ab-ffcbdf60d7d9",
      "metadata": {
        "id": "307e686b-5f41-471d-81ab-ffcbdf60d7d9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "terratorch_env",
      "language": "python",
      "name": "terratorch_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
